> In this study, Olsen et al. describe the development and application
> of sumrep, an R package that generates a variety of summaries of the
> features of different adaptive immune receptor sequence datasets, and
> that compares different datasets according to these summaries. The
> package is applied to empirical and simulated data, and demonstrated
> to be able to distinguish datasets that differ by major covariates
> such as twin relationships or research subject identity.

> The repertoire summary measures are produced by using a variety of
> well-known and already published packages, such as ape, alakazam and
> shazam, as well as new R functions. The package includes estimates of
> computationally intensive summaries, via subsampling and testing of
> estimate stability. Although the novelty of the biological findings in
> the manuscript is modest, (for example, showing that members of a pair
> of twins have more similar AIRR data to each other than to unrelated
> individuals) the wrapping of all of the disparate analysis approaches
> under one convenient R package umbrella will likely be of use and
> interest to other researchers in this specialized field within
> immunology. A consistent, relatively comprehensive tool of this kind
> could help to standardize analyses of complex AIRR data within the
> field. The authors' discussion of other tools in the literature is
> balanced.

> One major question is whether the kinds of AIRR summaries and
> comparisons in sumrep are able to provide meaningful insights into
> immunological questions that are less overt and dramatic than, for
> example, repertoire differences between identical twins compared to
> individuals with different germline genome sequences. For example, in
> Fig. 2, the overall differences between individuals in Coordinate 1 of
> the sumrep comparison are clear, but it is much less apparent whether
> the data from different timepoints before and after vaccination (when
> biologically important events are happening in T cell and B cell
> populations, but not necessarily ones that would perturb the entire
> repertoire) show any consistent trend with this kind of analysis.

The point of sumrep isn't that every summary we provide is useful for every application, but that we have summaries available for whatever application you have in mind, and that it is important to analyze the effectiveness of the summaries within the given context.
We do our best to analyze the informativeness of each summary using a few case studies, but it is difficult to make general conclusions about the summaries when there are so many different biological covariates each with different biological and physiochemical implications.

As for Figure 2, there is at least some signal in coordinate 2 with respect to donor-timepoint interactions for some summaries, for example, CDR3 lengths, D gene usage, DJ insertions, Hill numbers, V gene usage.


> Additional Comments:
> Page 2: "In addition to enabling comparison of different sequencing
> datasets, summary statistics can also be used to compare sequencing
> datasets to probabilistic models to which they have been fitted." and
> "Such generative models are used to simulate sequences as a 'ground
> truth' for benchmarking inferential software".
> The authors should consider expanding on the kinds of biological
> findings or hypothesis testing that could be enabled by more accurate
> modeling of AIRR sequence datasets, to increase the appeal of this
> aspect of the study to a broader immunology audience. If an accurate
> model is a black box that cannot be related to, for example, cellular
> processes and their regulation during VDJ recombination, or selective
> forces acting on B cells or T cells during the life of the organism,
> the appeal of this aspect of the project may be restricted to
> sub-specialists within this already specialized field.

The sort of summary-based model validation we introduce here is not only more widely applicable to other areas of immunology and biology, but the black-box nature of certain forms of statistical modeling is why we need this sort of model validation in the first place.
Understanding which summaries are, and, perhaps more importantly, are not captured, by a model gives insight into how the black box works.
Moreover, many of the summaries here can be applied to other repertoires of nucleotide sequences.

> For the experimental datasets used, it would be useful to know if the
> investigators have tested for other experimental covariates that could
> result in batch effects, for example, if different subjects' samples,
> or samples from different timepoints, were sequenced in different
> sequencer runs.

Testing for experimental covariates-- not sure what that means.
%MS It means whether protocol parameters, # of input cells/RNA, sequencing depts, etc can explain differences between samples
%BJO Perhaps Chaim and Mikhail can chime in here? I'm inclined to believe that the datasets were collected in a rigorous manner.

%MS: Repertoires for both donors (yellow fever vaccination dataset) were sequenced on the same lane, except for time point 0 which was sequenced separately for both donors. As Figure 2 shows clear separation of replicas for all time points, it is unlikely that any major bias was introduced by sequencing time point 0 separately. In our experience, lane-specific biases are related to cross-sample contamination and, as we have not computed repertoire overlaps for Figure 2, it is unlikely that any of such biases are present. On the other hand, biases related to V/D/J usage, spectratype, etc are quite common in cases when different library preparation and sequencing protocols are applied. However, yellow fever vaccination dataset was obtained using exactly the same protocol for all donors and time points.

> Table 1: Some further analysis and discussion of the degree of
> correlation between the different summary measures calculated from an
> AIRR dataset (beyond the mention on page 5 that such correlations
> exist), and how any such correlations could affect comparison between
> two different AIRR datasets, would help to put the analyses in
> perspective.

In fact, we do address the correlation between summaries via the "Ranking summary statistic informativeness" section, in which we employ a regularized least-squares approach to identify which summaries are most informative while controlling for correlation among summaries, and validate the method on a set of test datasets.


> Levenshtein distance: The authors could discuss whether insertions and
> deletions should be considered as comparable edit events in comparison
> to substitutions, when they are more rare in empirical AIRR data.

According to Yeap et al. (2015) (https://doi.org/10.1016/j.cell.2015.10.042), insertions and deletions are common, but often lead to sequences getting filtered out of the productive repertoire.
This makes it difficult to assess the exact level of rarity of indels, and how this might impact distances between sequences.
Nonetheless, it is not obvious to us how best to modify the Levenshtein (or similar) distance to account for relative rarity of  insertions/deletions vs substitutions, in particular how to choose appropriate weights for the respective event types.
To us it seems more principled to use an established, common metric versus attempting to construct an alteration based on rarity of events.

> Sequence-wise GC content: how useful is this summary? It seems like a
> measure imported from organismal phylogenetics, rather than something
> that would be often used in AIRR analysis

You're right that sequence-wise GC content is quite simple, which we enjoy having as a statistic that requires minimal processing.
Also, recall that SHM hotspots preferentially mutate at C/G bases.

In any case, if two different repertoires have significantly different GC content distributions, this hints that they may be significantly different in some more sophisticated characteristic.
Since GC content is very easy to compute, this seems like a good place to start when doing some exploratory data analysis on new datasets.

> One puzzling aspect is that the dataset availability statement for the
> manuscript states: "Generated Statement: The datasets analyzed in this
> manuscript are not publicly available. Requests to access the datasets
> should be directed to Branden Olson, branden.olson@gmail.com." This
> seems not to be the case. Many of the datasets referenced do appear to
> be in the SRA repository, such as those in references 6, 12, 29 and 32
> Lack of data sharing would make it impossible for other researchers to
> independently evaluate the reported results of the study and carry out
> additional analyses.

You are correct; the datasets from these four studies are all available via SRA.
We will include these SRA accession numbers, laid out in detail below, in the revision.

Britanova et al. 2016 (ref 6) -- SRA accession # PRJNA316572
Gupta et al. 2017 (ref 12) -- SRA accession # PRJNA349143
Pogorelyy et al. 2018 (ref 29) -- SRA accession # PRJNA493983
Rubelt et al. 2016 (ref 32) -- SRA accession # SRP065626
