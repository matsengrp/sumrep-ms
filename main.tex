\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[margin=1in]{geometry}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows}
\usepackage{adjustbox}
\tikzstyle{decision} = [diamond, draw, fill=blue!20,
        text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
        \tikzstyle{block} = [rectangle, draw, fill=blue!20,
                text width=6em, text centered, rounded corners, minimum height=4em]
                \tikzstyle{line} = [draw, -latex']
                \tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
                        minimum height=2em]

\graphicspath{ {figures/} }

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{exp(-((x-#1)^2)/(2*#2^2))}%
}

\pgfmathdeclarefunction{multimodal}{2}{%
  \pgfmathparse{0.7*exp(-((x-#1)^2)/(2))+0.3*exp(-((x-#2)^2)/(2))}%
}

\pgfplotsset{every tick label/.append style={font=\small}}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% Commands
\newcommand{\alphabet}{\mathcal{A}}

% http://bytesizebio.net/2013/03/11/adding-supplementary-tables-and-figures-in-latex/
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }

%BJO - Another possible title: A summary statistic framework for immune receptor repertoire comparison and model validation. 
\title{Immune receptor repertoire comparison and model validation via summary statistics}
\author{Branden Olson, Software WG folks, Frederick A Matsen IV}

\pgfplotsset{compat=1.12}
\begin{document}

\maketitle

\begin{abstract}
The adaptive immune system generates an incredible diversity of antigen receptors for B and T cells to keep dangerous pathogens at bay.
The DNA sequences coding for these receptors arise by a complex recombination process followed by a series of binding-based filters, as well as affinity maturation for B cells, giving considerable structure to the circulating pool of receptor sequences.
Although these data sets hold considerable promise for medical and public health application, the complex structure of the resulting immune repertoire sequencing (Rep-Seq) datasets makes analysis difficult.
In this paper we introduce \texttt{sumrep}, an R package that efficiently performs a wide variety of repertoire summaries and comparisons, and show how \texttt{sumrep} can be used to perform model validation.
%BJO Very high level summary here. Need to double check Peji and Chaim's analyses
We find that summaries vary in their ability to differentiate between datasets, although many are able to distinguish between donor as well as time of vaccination.
Furthermore, we find that state-of-the-art generative models excel at recapitulating gene usage and indel statistics in a given experimental repertoire, but struggle to capture many physiobiological properties of real repertoires.
\end{abstract}


\section*{Introduction}

B cells and T cells play critical roles in adaptive immunity through the cooperative identification of and response to antigens.
The random rearrangement process of the genes that construct B cell receptors (BCRs) and T cell receptors (TCRs) allows for the recognition of a highly diverse set of antigen epitopes.
We refer to the set of B and T cell receptors present in an individual's immune system as their immune receptor repertoire; this repertoire constantly changes over the course of an individual's lifetime for various reasons including differences in germline gene sets and antigen exposure.

Although immune receptor repertoires are now accessible for scientific research and medical applications through high-throughput sequencing, it is not necessarily straightforward to gain insight from and to compare these data sets.
%BJO The transition here feels awkward
Without further processing, repertoires are simply a list of DNA sequences.
After annotation one can use gene usage frequencies \cite{Hou2016-qc, Martin2015-ho, Corcoran2016-nw, Gadala2015-wq, Boyd2010-hd, Bolen2017-xt}, and then use some method to compare CDR3 sequences.
This can be a highly involved task, and so it is common to simply compare CDR3 length distribution of a repertoire \cite{Miqueu2007-lk,Larimore2012-lo}, leaving the full richness of CDR3 sequence unanalyzed, as well as other interesting aspects of the germline-encoded regions.
% or diversity measures of clonotype classifications \cite{Aouinti2015-ke, Bischof2016-fn}.
% Besides the theoretical complexity discussed above, experimental repertoire datasets are a very sparse sample of an individual's dynamic ``population'' repertoire, and these experimental samples are subject to sequencing error, truncated reads, and other sources of uncertainty and censorship.
% commenting because Dash isn't widely used for repertoire comparison
% Current approaches to repertoire comparison involve either distance-based methods \cite{Dash2017-rz},

An alternative strategy is to transform a repertoire to a more convenient space and compare the transformed quantities according to some metric.
For example, several studies reduce a set of nucleotide sequences to $k$mer distributions for classification of immunization status or disease exposure \cite{Madi2014-lt, Ostmeyer2017-xg, Heather2017pf}.
These $k$mer distributions can then be compared via a string metric, but still comprise a large space and lose important positional information.
One can perform other dimension reduction techniques like t-SNE to project repertoires down to an even smaller space \cite{Yokota2017-zm}, but these projections lose a lot of information and will have questionable immunological meaning.
%BJO I basically included the below sentence for the IMGT/HighV-Quest reference which runs on IMGT-particular data formats, for what it's worth
% Moreover, many of these comparison frameworks rely on a particular tool or pipeline, disallowing for general-purpose rep-seq dataset comparison.

We wish to facilitate the use of biologically interpretable summary statistics to capture many different aspects of Rep-Seq repertoires.
In addition to enabling comparison of different sequencing data sets, summary statistics can also be used to compare sequencing data sets to probabilistic models to which they have been fit.
Namely, one can use a form of model checking that is common in statistics: after fitting a model to data, one assesses the similarity of data generated from the model to the real data.
In this case, we generate a repertoire of sequences from the models and compare this collection to a real-data repertoire of sequences via summary statistics.

We are motivated to perform such comparison because these probabilistic models are used as part of inference, and because they are used for inferential tool benchmarking.
%EM Actually I'm not 100% positive that the inferential part will make sense to immunologists. So I'm not filling out this sentence yet.
For example...
Such generative models are used to simulate sequences to have a ``ground truth'' for benchmarking inferential software (refs), and thus the accuracy of such benchmarks depends on the realism of the generated sequences.
%EM Putting this reason on hold for the time being.
% Simulation tools can also be used to generate a null distribution to compare to data to look for a specific effect, such as natural selection \cite{Yaari2012-kk}.

Currently there are no unified packages dedicated to the task of calculating and comparing summary statistics for Rep-Seq data sets.
While the Immcantation pipeline (which includes the \texttt{shazam} and \texttt{alakazam} R packages) contains many summary functions for Rep-Seq data, it does not have general functionality for retrieving, comparing, and plotting these summaries \cite{Gupta2015-iu}.
Many summaries of interest are implemented in one package or another, but differences in functionality and data structures make it troublesome to compute and compare summaries across packages.
Some summaries of interest, such as the positional distance between mutations distribution, are not readily implemented in a package.

[Cartoon of projecting repertoires in various directions into real space, and comparing the resulting projections?]

In this paper, we gather dozens of meaningful summary statistics on repertoires, derive efficient and robust summary implementations, and identify appropriate comparison methods for each summary.
We present \texttt{sumrep}, an R package that computes these summary distributions for Rep-Seq datasets and performs repertoire comparisons based on these summaries.
We investigate the effectiveness of various summary statistics in distinguishing between different observed repertoires as well as between simulated and observed data, and show that most summaries are able to differentiate between subject as well as time after vaccination within a subject.
Further, we demonstrate how \texttt{sumrep} can be used for model validation through case studies of two state-of-the-art repertoire simulation tools: \texttt{IGoR} \cite{Marcou2018-du} for TCRs, and \texttt{partis} \cite{Ralph2016-nw, Ralph2016-iz} for BCRs.


\section*{Results}

\begin{table}
\makebox[\textwidth][c]{ % this centers a too-wide table
\begin{tabular}{l|c|c|c|l}
    Summary statistic & Annotations & Clustering & Phylogeny & Implementation
\\
\hline \hline
    Pairwise distance distribution & No & No & No & \texttt{stringdist} \cite{vanderLoo2014-re} \\
$k$th nearest neighbor distribution & No & No & No & \texttt{stringdist} \\
    GC-content distribution & No & No & No & \texttt{ape} \cite{Paradis04-hz} \\
    Hot/cold spot motif count distribution & No & No & No & \texttt{Biostrings} \cite{Pagas17-bi} \\
\hline
Distance from naive to mature distribution & Yes & No & No & \texttt{stringdist} \\
%BJO Another one we kind of skipped over.
$k$th nearest neighbor (V-sequence) distribution & Yes & No & No &\texttt{stringdist},  \texttt{sumrep} \\
CDR3 length distribution & Yes & No & No & Tool-provided \\
Joint distribution of germline gene use & Yes & No & No & \texttt{sumrep} \\
Pairwise CDR3 distance distribution & Yes & No & No & \texttt{stringdist} \\
    Hydrophobicity distribution & Yes & No & No & \texttt{Peptides} \cite{Osorio15-tv} \\
    Atchley factors distribution & Yes & No & No & \texttt{HDMD} \cite{McFerrin13-ng} \\
Aliphatic index distribution & Yes & No & No & \texttt{Peptides} \\
    G.R.A.V.Y. index distribution & Yes & No & No & \texttt{alakazam} \cite{Gupta2015-iu} \\
Per-gene substitution rate & Yes & No & No & Tool-provided + \texttt{sumrep} \\
Per-gene-per-position substitution rate & Yes & No & No & Tool-provided + \texttt{sumrep} \\
Per-base mutability model & Yes & No & No & \texttt{shazam} \cite{Gupta2015-iu} \\
Per-base substitution model & Yes & No & No & \texttt{shazam} \\
%BJO This one doesn't depend on fit-star, right?
%EM Yes, but we could just count Ts vs Tv, call it a "ratio" and drop the model-based rigor.
%EM Branden, I'm not actually sure what you ended up doing. Did you do this? If so, worth a line or two in the methods.
%BJO I believe I got caught up with which sequences to use for this, before we decided on assuming IMGT-gapped mature and germline sequences, which would now be the sensible default. So, this is not currently implemented.  
Transition/transversion ratio distribution & Yes & No & No & \texttt{sumrep} \\
Positional distance between mutations distribution & Yes & No & No & \texttt{sumrep}  \\
Distance from naive to mature distribution & Yes & No & No & \texttt{stringdist} \\
V/D/J deletion/insertion lengths distribution & Yes & No & No & Tool-provided \\
Transition matrix for insertions & Yes & No & No & \texttt{sumrep} \\
\hline
Cluster size distribution & Yes & Yes & No & Custom \\
Hill numbers (diversity indices) & Yes & Yes & No & \texttt{alakazam} \\
Selection estimates & Yes & Yes & No & \texttt{shazam} \\
%BJO blocked due to fit-star
Transition/transversion rates & Yes & Yes & No & Tool-provided \\
\hline
    Sackin index distribution & Yes & Yes & Yes & \texttt{CollessLike} \cite{Mir2018-lk} \\
Colless-like index distribution & Yes & Yes & Yes & \texttt{CollessLike} \\
Cophenetic index distribution & Yes & Yes & Yes & \texttt{CollessLike} \\
%BJO To be delineated
Graph-theoretical features & Yes & Yes & Yes & TBD \\
\end{tabular}
}
\caption{Working table of summary statistics grouped by their respective levels of assumed post-processing.}
\label{tab:SummaryStatistics}
\end{table}

\subsection*{Implementation}
The full \texttt{sumrep} package along with the following analyses can be found at \texttt{https://github.com/matsengrp/sumrep}.
It supports BCR and TCR repertoire datasets.
%EM Perhaps clarify what AIRR compliant means here? Did you ingest AIRR-formatted stuff for this manuscript?
It is open-source, unit-tested, and extensively documented, and uses default dataset fields and definitions that comply with AIRR rearrangement schema.
Analyses will be reproducible using Docker.

Table \ref{tab:SummaryStatistics} lists the summary statistics currently supported by \texttt{sumrep}, and includes the default assumed level of annotation, clustering, and phylogenetic inference for each summary.
The first group of statistics only requires the input sequences to be pairwise aligned and constrained to the variable region.
This coincides with the \texttt{sequence\_alignment} field in the AIRR schema.
The second group requires standard sequence annotations, such as inferred germline  ancestor sequences for Ig loci, germline gene assignments, and indel statistics.
Level three requires clonal family cluster assignments.
Level four requires an imputed phylogeny for each clonal family and is only relevant for Ig loci.
\texttt{sumrep} itself does not perform any annotation, clustering, or phylogenetic inference, but rather assumes such metadata are present in the given dataset; in principle, one can use any tool which performs these tasks as expected.

\texttt{sumrep} makes it easy to compare each summary statistic between two repertoires by employing a ``divergence'' appropriate for the summary in question.
For example, the \texttt{getCDR3LengthDistribution} function returns a vector of each sequence's CDR3 length, and the corresponding \texttt{compareCDR3LengthDistributions} function takes two repertoires and returns a numerical summary of the difference between these two length distributions.
The comparison method depends on the summary, and this is discussed more in the Methods section.
There is also a \texttt{compareRepertoires} function which takes two repertoires and returns as many summary comparisons as befits the data.

We have designed \texttt{sumrep} to efficiently approximate divergences between repertoires via subsampling, which is especially important for expensive-to-calculate summary statistics.
Computing full summaries can be computationally inefficient for even moderately sized repertoires.
When the target summary is a distribution, we can gain efficiency by repeatedly subsampling from the distribution until our estimate has stabilized.
The result is an approximation to the full distribution, which introduces some  extra noise but can yield very high efficiency gains for large datasets.
We outline a generic distribution approximation algorithm as well as a modified version for the nearest neighbor distance distribution in the Methods section, and conduct extensive empirical analyses of these algorithms in Appendices A and B.

The general framework of comparing summary statistics between repertoires $R_1$ and $R_2$ is illustrated in Figure~\ref{fig:DivergenceCartoon}.
A given summary $s$ is applied separately to $R_1$ and $R_2$, which for most summaries yields a distribution of values.
These two resultant distributions can be compared using a divergence $d$ that is tailored to the nature of $s$.
We use Jenson-Shannon (JS) divergence to compare scalar distributions (e.g. GC content, CDR3 length), and the $\ell_1$ divergence to compare categorical distributions (e.g. gene call frequencies, amino acid frequencies).
%EM Shall we try for an intuitive explanation of JS / KL?
%BJO I like this description from https://medium.com/@cotra.marko/making-sense-of-the-kullback-leibler-kl-divergence-b0d57ee10e0a:
% [It's] a measure of how much ?predictive power? or ?evidence? each sample will on average bring when you?re trying to distinguish p(x) from q(x), if you?re sampling from p(x).
%BJO - Then the JS divergence will be a nice, symmetrized version. 
%BJO - However, you have a better idea of how much stats background the average immunologist will have.

\begin{figure}
\begin{gather}
s(R_1) =
\begin{tikzpicture}[baseline={(0.5, 0.5)}]
\begin{axis}[
  domain=0:8, samples=100,
  xlabel near ticks,
  xtick=4,
  yticklabels={,,},
  xticklabels={GC content},
  tick style={draw=none},
  axis line style={draw=none},
  height=3cm, width=3cm,
  enlargelimits=false, clip=false, axis on top
  ]
  \addplot [very thick,orange!90!black, fill=orange!20!] {multimodal(3,5.5)};
\end{axis}
\end{tikzpicture}
\hspace{1em}
s(R_2) =
\begin{tikzpicture}[baseline={(0.5, 0.5)}]
\begin{axis}[
  no markers, domain=0:11, samples=100,
  xlabel near ticks,
  xtick=5.5,
  yticklabels={,,},
  xticklabels={GC content},
  tick style={draw=none},
  axis line style={draw=none},
  height=3cm, width=3cm,
  enlargelimits=false, clip=false, axis on top
  ]
  \addplot [very thick,violet, fill=violet!20!] {multimodal(3, 6)};
\end{axis}
\end{tikzpicture}
\\
d
\left(
	\begin{tikzpicture}[baseline={(0.5, 0.5)}]
		\begin{axis}[
  		domain=0:8, samples=100,
      	axis lines=none,
      	height=3cm, width=3cm,
      	enlargelimits=false, clip=false, axis on top
      	]
      	\addplot [very thick,orange!90!black, fill=orange!20!] {multimodal(3,5.5)};
    	\end{axis}
    \end{tikzpicture}
,
\begin{tikzpicture}[baseline={(0.5, 0.5)}]
\begin{axis}[
  no markers, domain=0:11, samples=100,
  axis lines=none,
  height=3cm, width=3cm,
  enlargelimits=false, clip=false, axis on top
  ]
  \addplot [very thick,violet, fill=violet!20!] {multimodal(3, 6)};
\end{axis}
\end{tikzpicture}
\right)
= 0.05
\end{gather}
\caption{
Cartoon of our summary divergence framework.
Most summary statistics $s$, e.g.\ GC content, yield a distribution of values when applied to each of the sequences in a given repertoire $R$.
We can compare these distributions using a statistical divergence $d$, which takes two distributions and outputs a nonnegative scalar.
We have also chosen appropriate divergences for summaries that do not give distributions (such as substitution rates and diversity indices).
}
\label{fig:DivergenceCartoon}
\end{figure}

\texttt{sumrep} also contains a plotting function for each univariate summary distribution.
For example, the \texttt{getCDR3LengthDistribution} comes with another companion plotting function called \texttt{plotCDR3LengthDistribution}.
\texttt{sumrep} also comes with a ``master'' plotting function, \texttt{plotUnivariateDistributions}, which shows a gridded figure of univariate distribution plots relevant to the locus in question.
Currently, these plotting functions support frequency polygons and empirical cumulative distribution functions (ecdfs).
%EM referring to figures out of order often raises red flags with reviewers. Shrug.
%BJO We could say something like "Examples of these plots are given in later sections" without actually including explicit refs? It seems "wasteful" to include a separate plot here just to illustrate something that is later illustrated, but it seems wrong to mess with the plot reference numbers as they are.
Figure~\ref{fig:PartisFreqPolys} shows an example gridded frequency polygon plot from \texttt{plotUnivariateDistributions} computed for a set of six BCR datasets (experimental and observed), and \ref{fig:PartisECDFs} shows ecdfs for the same datasets.

%BJO - Chaim's and Peji's analyses
\subsection*{Empirical analysis of summary divergences by individual for IgH sequences}

\subsection*{Multidimensional scaling of TCR sequences by individual}
To examine the ability for each summary statistic to distinguish between differences in repertoires across individuals and time points, we performed a multidimensional scaling (MDS) analysis of summary divergences.
In particular, we computed divergences of each summary between each pair of a set of datasets stratified by two individuals and five timepoints post-vaccination to form a dissimilarity matrix.
We then mapped these dissimilarity matrices to an abstract Cartesian space using MDS.
Figure~\ref{fig:TCR_MDS} displays plots of the first two coordinates of each space grouped by donor and timepoint.
\begin{figure}
    \includegraphics[width=\linewidth]{Figures/tcr_pca.pdf}
    \caption{PCA plots of select summaries by donor and timepoint}
    \label{fig:TCR_MDS}
\end{figure}
We see that for almost all summaries, these points cluster by donor, with the CDR3 pairwise distance distribution being the only summary which does not decisively cluster according to donor identity.
%EM I'm not totally convinced that the cross is more than just random noise...
The D gene usage distribution has more unusual cross-like pattern, whose structure  is more difficult to assess.
Other summaries, such as the CDR3 pairwise distance distribution and Hill numbers, yield little to no signal.
Moreover, there does seem to be signal induced by donor-timepoint interactions, although the tightness of clustering varies by summary, with some summaries (e.g. DJ insertion length distribution) being tightly clustered by a given donor/timepoint value and some summaries (e.g. Kidera factor 4) not obviously clustering by donor/timepoint
Although these patterns would require further exploration in a particular research context, there seem to be interesting dynamics underlying \texttt{sumrep} divergences when covariates differ between datasets.

\subsection*{Comparing observations to model simulations per model}
\texttt{sumrep} can also be used to validate BCR/TCR generative models, i.e.\ models from which one can generate (simulate) data.
Given a repertoire dataset, we infer model parameters using software based on the given model, and use these parameters to generate corresponding simulated datasets.
Next, we use \texttt{sumrep} to compute each summary statistic listed in Table \ref{tab:SummaryStatistics} for each dataset, and compare these summaries between each pair of datasets.
We score how well the software's simulation replicates a given summary based on how small the divergences of observed/simulated dataset pairs are compared to divergences between arbitrary observed/observed or simulated/simulated pairs.
More details can be found in the Methods.

Applying this methodology to many datasets gives a clear view of characteristics which the model captures well, as well as areas for improvement.
As described in the introduction, we are motivated to do this because models are often benchmarked on simulated data, and it is important to understand discrepancies between simulated and observed data in order to properly interpret and extrapolate benchmarking results.
We emphasize that validating the model in this way is different than the usual means of benchmarking model performance-- rather than benchmarking the inferential results of the model, we benchmark the model's ability to generate realistic sequences.

We illustrate this approach with two case studies:
an analysis of IGoR \cite{Marcou2018-du} applied to TCR sequences, and an analysis of \texttt{partis}\cite{Ralph2016-nw, Ralph2016-iz} simulations applied to BCR sequences.

%EM Agreed here!
%BJO - TODO: Add quick summaries of the IGoR and partis analyses as summarized form the Methods subsections

\subsection*{Assessing summary statistic replication for \texttt{igor}}
Figure~\ref{fig:ObsScoresTCR} displays the observation-based summary scores for the six TCR datasets based on \texttt{igor} simulations.
We exclude \texttt{sequence\_alignment}-based summaries such as GC content and pairwise distance distributions since they are likely compounded by differences in the read lengths of the query sequences versus the full variable region nucleotide sequences of the simulations, and IGoR does not currently have an option to output the full variable region nucleotide sequences for experimental reads.
%EM Wow, this is pretty interesting that igor has trouble with some simple things like J usage. Can you think of why that might be?
%BJO Oops -- I inverted these scores recently so that a high positive score means 'good', i.e. a better summary replication. And apparently didn't fully change this everywhere in the ms.
\begin{figure}
	\begin{subfigure}{\textwidth}
    	\includegraphics[width=\linewidth]{Figures/IgorScores/obs_score_plot.pdf}
    	\caption{$\text{score}_\text{obs}$ of each statistic based on \texttt{igor} model inference and simulation.
        	A high LRD indicates a well-replicated statistic by the simulations.
    	}
    	\label{fig:ObsScoresTCR}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
    	\includegraphics[width=\linewidth]{Figures/IgorScores/sim_score_plot.pdf}
    	\caption{$\text{score}_\text{sim}$ of each statistic based on \texttt{igor} model inference and simulation.
    	    A high LRD indicates a well-replicated statistic by the simulations.
    	}
    	\label{fig:SimScoresTCR}
	\end{subfigure}
\end{figure}
It is seen that IGoR simulations are able to recapitulate gene usage statistics of an empirical repertoire very well, with J gene usage frequency being the most accurately replicated, followed by various indel length statistics.
V, D, and joing VDJ gene usage are all also well-replicated, as well as both VD and DJ insertion matrices.
The CDR3 length distribution is the least-replicated statistic, although the level of discrepancy is perhaps compounded by training IGoR on both functional and nonfunctional sequences.
Interestingly, the Kidera factors of the junction region are also replicated well, despite CDR3 length being one of the least accurately replicated statistics.
Other junction-based statistics besides Kidera factors range from mildly good to mildly bad replication, with the GRAVY index distribution being the best junction-based statistic (excluding Kidera factors).

Figure~\ref{fig:SimScoresBCR} displays the simulation-based summary scores for the same datasets and simulations.
We still see high scores for gene usage and indel statistics, although now the various Kidera factor and GRAVY index distributions have lower scores.
This suggests that while the average \texttt{igor} simulation yields Kidera factor and GRAVY index distributions that look more like the observed repertoire's distributions than other observed repertoires do, these simulated repertoires still tend to produce similar distributions to each other, as seen in Figure~\ref{fig:IgorFreqPolys}.
\begin{figure}
    \includegraphics[width=\linewidth]{Figures/IgorScores/igor_freqpoly.pdf}
    \caption{Frequency polygon plots of each univariate summary distribution for the IGoR datasets.}
    \label{fig:IgorFreqPolys}
\end{figure}

\begin{figure}
    \includegraphics[width=\linewidth]{Figures/IgorScores/igor_ecdf.pdf}
    \caption{Empirical cumulative distribution function plots of each univariate summary distribution for the IGoR datasets.}
    \label{fig:IgorECDFs}
\end{figure}



\subsection*{Assessing summary statistic replication for \texttt{partis}}
Figure~\ref{fig:ObsScoresBCR} displays the observation-based summary scores for the six BCR datasets based on \texttt{partis} simulations.
%EM Now I think I'm confused. Partis has a low score for pos dist btw muts, meaning that it fits this distribution very well? It doesn't look great in the other plots...
%BJO Addressed in above response
\begin{figure}
	\begin{subfigure}{\textwidth}
    	\includegraphics[width=\linewidth]{Figures/PartisScores/obs_score_plot.pdf}
    	\caption{$\text{score}_\text{obs}$ of each statistic based on \texttt{partis} model inference and simulation.
     	   A high score indicates a well-replicated statistic by the simulations.
    	}
    	\label{fig:ObsScoresBCR}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
    	\includegraphics[width=\linewidth]{Figures/PartisScores/sim_score_plot.pdf}
    	\caption{$\text{score}_\text{sim}$ of each statistic based on \texttt{partis} model inference and simulation.
    	    A high score indicates a well-replicated statistic by the simulations.
    	}
    	\label{fig:SimScoresBCR}
	\end{subfigure}
\end{figure}
Like \texttt{igor}, we see that \texttt{partis} simulations also excel at replicating gene usage and indel statistics, while also replicating CDR3 length distributions well, but failing to recapitulate VD and DJ insertion matrices.
This contrasts \texttt{igor} which incorporates these insertion matrices during model fitting, and thus recapitulates these matrices well.
The other statistics yield scores ranging from slightly to very negative, with many mutation-related summaries like positional distance between mutations, and hot and cold spot counts, not being well-captured.
This is not very surprising as \texttt{partis} does not incorporate these quantities into the model fitting process, but does suggest that these sorts of quantities may need to be more explicitly accounted for in BCR generative models.


Figure~\ref{fig:SimScoresBCR} displays the simulation-based summary scores for the same datasets and simulations.
The scores are highly similar to those seen in \ref{fig:ObsScoresBCR}, with some summaries like positional distance between mutations and VD/DJ insertion matrices seeing a moderate drop.

\begin{figure}
%EM I think that only one of these should go in the main text, but it's a matter for discussion with the group.
    \includegraphics[width=\linewidth]{Figures/PartisScores/partis_freqpoly.pdf}
    \caption{Frequency polygon plots of each univariate summary distribution for the \texttt{p\_f1}, \texttt{p\_f1\_sim}, \texttt{p\_g1}, and \texttt{p\_g1\_sim} datasets.}
    \label{fig:PartisFreqPolys}
\end{figure}

\begin{figure}
    \includegraphics[width=\linewidth]{Figures/PartisScores/partis_ecdf.pdf}
    \caption{Empirical cumulative distribution function plots of each univariate summary distribution for the \texttt{p\_f1}, \texttt{p\_f1\_sim}, \texttt{p\_g1}, and \texttt{p\_g1\_sim} datasets.}
    \label{fig:PartisECDFs}
\end{figure}


\section*{Methods}
\subsection*{Divergence}
We elect the Jenson-Shannon (JS) divergence for most of our comparison functions.
The Jenson-Shannon divergence of probability distributions $P$ and $Q$ with densities $p(\cdot)$ and $q(\cdot)$ is a symmetrized Kullbeck-Leiber divergence, defined as
\begin{equation}
\text{JSD}\left(P || Q\right) := \frac{\text{KLD}\left(P || M\right) + \text{KLD}\left(Q || M\right)}{2}
\end{equation}
where $M := (P + Q)/2$ and $\text{KLD}(P || M)$ is the usual KL-divergence,
\begin{equation}
\text{KLD}\left(P_1 || P_2\right) := \operatorname{E}_{\mathbf X \sim P_1}\left[ \log\left(\frac{p_1(\mathbf X)}{p_2(\mathbf X)}\right) \right].
\end{equation}
In the case where $P$ and $Q$ are both discrete distributions, this becomes
\begin{equation}
\text{KLD}\left(P_1 || P_2\right) = \sum_{n \in \Omega} p_1(n) \log\left( \frac{p_1(n)}{p_2(n)} \right)
\end{equation}
where $\Omega$ is the countable sample space for $P_1$.
Because the discrete formulation is much nicer to worth with than the continuous one, we discretize continuous samples and treat them as discrete data.
By default, we use $B = \max\left(\left\lceil \sqrt{\min(m, n)} \right \rceil, 2\right)$ bins of equal length, which scales reasonably with $m$ and $n$ simultaneously.
We also discard bins which would lead to an infinite KL divergence to increase numerical stability.

Sometimes it makes more sense to use other distance metrics, such as the sum of absolute differences, or $\ell_1$ divergence, for count or categorical data:
\begin{equation}\label{eq:SAD}
    d_{\ell_1}(R_1, R_2; c, \mathcal S) = \sum_{s \in \mathcal S} \left| c(s; R_1) - c(s; R_2) \right|.
\end{equation}
In words, \eqref{eq:SAD} iterates over each element $s$ in some set $\mathcal S$, calculates the count $c$ of $s$ within repertoires $R_1$ and $R_2$ respectively, takes the absolute difference of counts, and appends this to a rolling sum.
This metric is well suited for comparing marginal or joint V/D/J-gene usage distributions:
if $\mathcal V$, $\mathcal D$, and $\mathcal J$ represent the germline sets of V, D, and J genes, respectively,
defining define usage $u$ of gene triple $(v, d, j) \in \mathcal V \times \mathcal D \times \mathcal J$ for repertoire $R$ as
\begin{equation}
u(R; v, d, j) = \#\left\{s \in R: s_v = v \land s_d = d \land s_j = j\right\},
\end{equation}
then our comparison metric of joint VDJ gene usage for repertoires $R_1$ and $R_2$ becomes
\begin{equation}
d(R_1, R_2; u, \mathcal V, \mathcal D, \mathcal J) = \sum_{v \in \mathcal V} \sum_{d \in \mathcal D} \sum_{j \in \mathcal J} \left| u(v, d, j; R_1) - u(v, d, j; R_2) \right|.
\end{equation}
This metric is also nice for computing amino acid frequency and 2mer frequency distributions.
Note that we can normalize the counts to become frequencies and apply the $\ell_1$ metric on the resultant scale which may be better suited to the application, especially when dataset sizes differ notably.

\subsection*{Approximating distributions via subsampling and averaging}
Computing full summary distributions over large datasets can be intractable.
However, we can compute a sort of Monte Carlo distribution estimate by repeatedly subsampling and aggregating summary values.
This idea is laid out in Algorithm~\ref{DistributionAveraging}, which appends subsamples of $d$ to a rolling approximate distribution until successive distributions are sufficiently similar.
Particularly, the algorithm terminates when successive distribution iterates have a sufficiently small JS divergence.
Note that continually appending values to a rolling vector is analogous to computing a rolling average, where the subject of the averaging is an empirical distribution rather than a scalar.
\begin{algorithm}
    \caption{Compute automatic approximate distribution\\
        \textbf{Input:} repertoire $R$, summary $s$, subsample size $m$, convergence tolerance $\varepsilon$\\
        \textbf{Output:} subsampled approximation to $d$}
    \label{DistributionAveraging}
    \begin{algorithmic}
        \State $R_0 \gets \text{subsample}(R, m)$
        \State $d_0 \gets s(R_0)$
        \State $n \gets 1$
        \State error $\gets \infty$
        \While{error $> \varepsilon$}:
        \State $R_\text{samp} \gets \text{subsample}(R, m)$
        \State $d_\text{samp} \gets s(R_\text{samp})$
        \State $d_n \gets \text{concatenate}(d_{n-1}, d_\text{samp})$
        \State error $\gets \text{JSD}(d_{n-1}, d_n)$
        \State $n \gets n + 1$
        \EndWhile
    \end{algorithmic}
    \Return $d_n$
\end{algorithm}

An alternative would be to simply compute the distribution on one subsample of the data and use this as an approximate distribution.
The main advantage of Algorithm \ref{DistributionAveraging} over such an approach is that it provides a sense of how close the approximation is to the full distirbution via the tuning parameter $\varepsilon$, while automatically determining the size of the subsample.
We conduct a performance analysis of Algorithm \ref{DistributionAveraging} in Appendix A and empirically demonstrate efficiency gains in a variety of realistic settings.

There are some summaries which induce distributions for which Algorithm \ref{DistributionAveraging} is ill-suited.
This occurs when a subset of the summary does not follow the same distribution as the full summary.
For example, consider the nearest neighbor distance of a sequence $s_i$ to a repertoire of sequences $R$,
\begin{equation}
d_\text{NN}(s_i, R) := \min_{s \in R \setminus \{s_i\}} d(s_i, s),
\end{equation}
where $d(\cdot, \cdot)$ is any string metric, such as the Levenshtein distance.
If we take any subset $S$ of $R$, then $d_\text{NN}(s_i, S) \ge d_\text{NN}(s_i, R)$ $\forall i$, since $R$ will have the same sequences to iterate over, and possibly more sequences, which can only result in the same or a smaller minimum.

In this case, we can still approximate the nearest neighbor distance distribution using the following modification, which is explicitly described in Algorithm \ref{NNDistributionAveraging}.
For each iteration, sample a small batch $B = (s_1, \dotsc, s_b)$ of $b$ sequences (e.g. $b = 50$), and compute $d_\text{NN}$ of each $s_i$ to the full repertoire $R$.
This will give an unbiased sample $d_\text{NN}(s_1, R), \dotsc, d_\text{NN}(s_b, R)$ of the true nearest neighbor distance distribution $D_\text{true}$ of $R$, but it may have significant noise depending on the nature of $D_\text{true}$ and the batch size $b$.
Thus, we append batches to a running distribution until convergence as in Algorithm \ref{DistributionAveraging}, which produces more refined approximations as the tolerance decreases.

Algorithm \ref{NNDistributionAveraging} may yield a high runtime if $R$ is large, the sequences in $R$ are long, or the tolerance is small.
Nonetheless, we empirically demonstrate in Appendix B that in the case of typical BCR sequence reads, even very small tolerances incur reasonable runtimes, and when $R$ is large, the algorithm can be more than 100 times faster than computing the full distribution on $R$.

\begin{algorithm}
    \caption{Compute automatic approximate nearest neighbor distance distribution\\
        \textbf{Input:} repertoire $R$, distance $d$, subsample size $m$, convergence tolerance $\varepsilon$\\
        \textbf{Output:} subsampled approximation to $d$}
    \label{NNDistributionAveraging}
    \begin{algorithmic}
        \State $d_0 \gets \Call{doBatchStep}{R, m}$
        \State $n \gets 1$
        \State error $\gets \infty$
        \While{error $> \varepsilon$}:
        	\State $d_\text{samp} \gets \Call{doBatchStep}{R, m}$
        	\State $d_n \gets \text{concatenate}(d_{n-1}, d_\text{samp})$
        	\State error $\gets \text{JSD}(d_{n-1}, d_n)$
        	\State $n \gets n + 1$
        \EndWhile
            \Return $d_n$
    \end{algorithmic}
    \begin{algorithmic}
    \Function{doBatchStep}{$R, m$}
    \For{$i = 1, \dots, m$}:
		\State $s_i \gets \text{subsample}(R, 1)$
        \State $d_i \gets d_\text{NN}(s_i; R)$
	\EndFor
	\Return $(d_1, \dotsc, d_m)$
	\EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection*{Model validation of \texttt{IGoR} and \texttt{partis}}
Since \texttt{partis} returns a list of the top most probable scenarios for each rearrangement event, we use only the most likely for each sequence.
Then, we compute the comparisons or divergences between each observed-simulated pair as well as between each of the observed datasets.
For each comparison, we subsample one receptor per clonal family to get a dataset consisting of ``unique clones'', for both the observed and simulated datasets.
We do this since \texttt{partis simulate} draws from distributions over clonal families for each rearrangement event as inferred from \texttt{partis partition}.
While it is possible to simulate multiple leaves for each rearrangement, it is not obvious how to best synchronize this with the observed clonal family distributions.
For example, if we had one clonal family with 1,000 members, and 99 singleton clonal families, we could try to coerce the simulator to produce on average a (roughly) size-1,000 clonal family for every 99 singletons.
However, it seems like this is highly contrived to the nature of our experimental sample, and does not clearly mimic the true repertoire dynamics of the individual from which the dataset was obtained.
Also, there is ambiguity to whether the rearrangement parameters for this size-1,000 clonal family should always match those of the observed size-1,000 clonal family, or if they should be sampled in some random fashion.
Hence, it seems more principled to subsample to unique clones and examine clonal family dynamics without dealing with abundance biases.

Next, we discuss a similar model validation approach for TCR repertoires using \texttt{IGoR} \cite{Marcou2018-du}.
Although IGoR is typically applied to non-productive sequences in order to capture the pre-selection recombination process, for this example application we wished to understand IGoR's ability to fit the complete repertoire directly without the need for an additional selection model (e.g.\ \cite{Elhanati2014-mf}).

We applied IGoR in this way to six datasets from \cite{Britanova2016-iw}, which studied T cell repertoires from donors ranging from newborn children to centenarians.

\subsection*{Scoring summary statistic replication by model}
%EM Would it be worth using some of these figures https://github.com/matsen/talks/tree/gh-pages/figures/sumrep ? (I just invited you.) I think that we're used to this framework but it may be a little strange for others.
%BJO Yes! Though I might have to put this off until after the WG meeting.
We would like a measure of how well a given statistic is replicated when a model performs simulations using parameters inferred from an observed repertoire dataset.
One approach is to score the statistic $s$ based on the average divergence of observations to their simulated counterparts when applying $s(\cdot)$, and the average divergence of observations to other observations when applying $s(\cdot)$.
Suppose we have $k$ different experimental repertoires of immune receptor sequences, and let $R_{i, \text{obs}}$ and $R_{i, \text{sim}}$ denote the $i$th observed and simulated repertoire, respectively.
For a given statistic $s$, let $\mathcal D(R_1, R_2; s)$ be the divergence of repertoires $R_1$ and $R_2$ with respect to $s$.
We can score a simulator's ability to recapitulate $s$ from the observed repertoire to the simulated via
\begin{equation}
    \text{score}_\text{obs}(s) :=
    \log \left(
        \frac{
            \frac{1}{\frac{1}{2} k\left(k - 1\right)}
            \sum_{i=1}^{k}
            \sum_{j \ne i}
                \mathcal D\left(R_{i, \text{obs}}, R_{j, \text{obs}}; s\right)
        }
        {
            \frac{1}{k}
            \sum_{i = 1}^k
                \mathcal D \left( R_{i, \text{obs}}, R_{i, \text{sim}} ; s\right)
        }
    \right).
\end{equation}
For a given summary $s$, score$_\text{obs}$ will be positive if the simulated repertoires tend to look more like their experimental counterparts in terms of this summary than experimental repertoires look like other experimental repertoires, and negative if experimental repertoires tend to look more like other experimental repertoires than they do their simulated counterparts.
In other words, this scores how well a simulator can differentiate $s$ from an experimental repertoire among other repertoires, and recapitulate $s$ into its simulation.

Another related score would be comparing the average divergence of observations to their simulated counterparts, and the average divergence of simulations to other simulations.
Formally, this is
\begin{equation}
    \text{score}_\text{sim}(s) :=
    \log \left(
        \frac{
            \frac{1}{\frac{1}{2} k\left(k - 1\right)}
            \sum_{i=1}^{k}
            \sum_{j \ne i}
                \mathcal D\left(R_{i, \text{sim}}, R_{j, \text{sim}}; s\right)
        }
        {
            \frac{1}{k}
            \sum_{i = 1}^k
                \mathcal D \left( R_{i, \text{obs}}, R_{i, \text{sim}} ; s\right)
        }
    \right).
\end{equation}
This value for a given summary will be negative if simulated repertoires tend to look more like their experimental counterparts (small $\mathcal D$ on the denominator) in terms of this summary than simulated repertoires look like other simulated repertoires, and positive if the simulated repertoires tend to look more alike.

We apply this approach to \texttt{partis} and \texttt{igor} simulations in the Results section.
However, this framework can be used to validate any immune receptor repertoire simulator which outputs the fields compatible with the summaries in \ref{tab:SummaryStatistics}, or more generally any set of summaries outside of the scope of \texttt{sumrep}.

A feature of our methodology is that we use the same tool to produce simulations that we used to produce the annotations.
To examine the sensitivity of this method, we performed a separate analysis by obtaining dataset annotations from standalone \texttt{igblast} \cite{Ye2013-kl}, and comparing these to simulations based on \texttt{partis} annotations using the \texttt{igblast} germline databases.
This is discussed in detail in Appendix C.


\subsection*{Materials}
The raw data for the BCR model validation analyses were obtained from \cite{Laserson2014-dx}, and the processed data, courtesy of Jason, are from a paper that isn't out yet.
These datasets represent repertoires of patients at various time points following an influenza vaccination.

For the TCR model validation analysis, we use several datasets from \cite{Britanova2016-iw}.
For tractability purposes, we chose the six datasets with the fewest number of sequence reads; the number of reads in these six datasets ranged from 37,363 sequences to 243,903 sequences.
%BJO - Trying to run \texttt{igor} on larger datasets would often throw a segmentation fault. If this is a concern, we could choose a dataset from each of five individuals and subsample each dataset to, say, 50,000 sequences. But I don't think this would change the result that much.
These datasets consist of consensus RNA sequences assembled using UMIs.
Most of these sequences are functional; as previously described, for this example application we are benchmarking IGoR's ability to fit complete repertoires rather than only non-functional repertoires as described above.

\section*{Discussion}
We have presented a general framework for summarizing and comparing Rep-Seq datasets, and applied it to several questions of scientific interest.
One can imagine many further applications of \texttt{sumrep}, as well as general avenues of research.

A natural extension of the model validation in this report would be to assess the performance of many competing repertoire analysis tools over a larger group of datasets.
Moreover, it would likely be useful to perform separate analyses restricted to different CDRs and framework regions, as physiochemical characteristics of these regions can be quite different.


Contrasting repertoires in the context of antigen response or vaccination design and evaluation may shed some light on which summaries can distinguish between such covariates.
\texttt{sumrep} could also be used to evaluate the extent to which artificial lymphocyte repertoires look like natural ones\cite{Finlay2012}.


\bibliographystyle{plain}
\bibliography{main}

\beginsupplement

\section*{Appendix A: Performance of distribution subsampling algorithms}
Here, we run Algorithm \ref{DistributionAveraging} on \texttt{p\_f1} subsampled without replacement to 10,000 sequences.
We compute the pairwise distance distribution of CDR3 sequences for the full subsampled dataset, as well as using Algorithm \ref{DistributionAveraging} with tolerances $\varepsilon \in \left\{0.1, 0.001, \dotsc, 10^{-7} \right\}$.
We replicate this experiment for 10 trials so that the subsampled dataset remains the same, but a new instance of the subsampling algorithm is run each time.
\begin{figure}
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/PairwiseDistance/freqpoly_by_tol.pdf}
   		\caption{Frequency polygons of true and subsampled pairwise distance distributions by tolerance.}
    	\label{fig:FreqPoly}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/PairwiseDistance/ecdf_by_tol.pdf}
    	\caption{Empirical c.d.f. of true and subsampled pairwise distance distributions by tolerance.}
    	\label{fig:ECDF}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/PairwiseDistance/div_by_tol.pdf}
    	\caption{KL-divergence to true pairwise distance distribution by tolerance, taken over 10 trials of the algorithm.}
    	\label{fig:Divergences}
	\end{subfigure}
    \begin{subfigure}{.5\textwidth}
    	\includegraphics[width=0.9\linewidth]{Figures/PairwiseDistance/log_time_by_tol.pdf}
    	\caption{Runtime (in seconds) and log-runtime (in log-seconds) for Algorithm \ref{DistributionAveraging} by tolerance, taken over 10 trials.}
    	\label{fig:Times}
    \end{subfigure}
    %EM if we want these to get a "Figure XX" label the overall figure needs a caption.
    %BJO I don't believe I needed/wanted these sorts of labels for references, but I'm not against it either.
    \caption{Performance of distribution subsampling algorithms.}
\end{figure}
Figure~\ref{fig:FreqPoly} shows a frequency polygon of the same distributions, and Figure~\ref{fig:ECDF} shows their empirical cumulative distribution functions.

Figure~\ref{fig:Divergences} displays the KL-divergence to the true distribution for each tolerance.
\begin{figure}

\end{figure}
Figure~\ref{fig:Times} displays the runtimes and log-runtimes for each tolerance as well as the true "population" runtime for the full dataset.


For a tolerance of $\varepsilon = 10^{-3}$, we see that the KL-divergence of the approximate distribution to the true distribution is consistently small, most of the time falling below 0.01.
Figure~\ref{fig:Times} shows that running the algorithm with this tolerance yields an exponentially faster runtime than when computing the full distribution.
Even the higher tolerances of $\varepsilon = 0.1, 0.01$ yield low KL divergences, although the variability is higher.

Next we investigate the effect of dataset size on the performance of Algorithm \ref{DistributionAveraging}.
For sample sizes $s \in \{\exp(5), \dotsc, \exp(9)\}$, we subsample \texttt{p\_f1} without replacement to $s$ sequences and compute the pairwise distance distribution of CDR3 sequences for the full subsampled dataset as well as those given by tolerances $\varepsilon \in \{0.1, 0.01, 0.001\}$.
We perform this experiment 10 times for each $s$.
Boxplots of the KL-divergence by log(size) and tolerance over all trials are displayed in Figure~\ref{fig:PDDivBySize}.
We see no obvious trend in the effect of dataset size on the KL-divergence for any choice of tolerance for the pairwise distribution.
Boxplots of the runtime (in log-seconds) by log(size) and tolerance are shown in Figure~\ref{fig:PDTimeBySize}, and boxplots of the log-efficiency by log(size) and tolerance are shown in Figure~\ref{fig:PDEfficiencyBySize}, where we define the efficiency to be
\begin{equation}\label{eq:Efficiency}
	\text{Efficiency} :=
		\frac{\text{time to compute the full distribution}
		}{
			  \text{time to compute the approximate distribution}
		}.
\end{equation}
Note the log-log scale, so that the line $y=0$ corresponds to instances when the true and approximate routines have identical runtimes.
Thus, the region $y > 0$ corresponds to instances when Algorithm \ref{DistributionAveraging} outperforms the computation of the full nearest neighbor distribution.
We see that while runtime increases superlinearly with dataset size for each tolerance, the efficiency appears to increase exponentially with dataset size.
Thus, the accuracy of Algorithm \ref{DistributionAveraging} applied to the pairwise distance distribution is scalable to large datasets while leading to large gains in runtime efficiency.

\begin{figure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/PairwiseDistance/div_by_size_and_tol.pdf}
        \caption{KL-divergence to true pairwise distance distribution by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
        \label{fig:PDDivBySize}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/PairwiseDistance/time_by_size_and_tol.pdf}
        \caption{Runtime by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
        \label{fig:PDTimeBySize}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/PairwiseDistance/efficiency_by_size_and_tol.pdf}
        \caption{Efficiency by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
        \label{fig:PDEfficiencyBySize}
    \end{subfigure}
\end{figure}

Finally, we investigate the effect of summary statistic on the performance of Algorithm \ref{DistributionAveraging}.
We run the algorithm for the pairwise distance, GC content, hotspot count, and coldspot count distributions on \texttt{p\_f1} subsampled without replacement to 5,000 rows.
For each summary, we run the algorithm for tolerances $\varepsilon \in \{0.1, \dotsc, 10^{-7}\}$.
We perform this experiment 10 times for each (summary, $\varepsilon$) combination.
Figures \ref{fig:DivBySummary}, \ref{fig:TimeBySummary}, and \ref{fig:EfficiencyBySummary} show the KL-divergence to the full dataset distributions, runtimes, and efficiencies, respectively, by summary and tolerance over all trials.
\begin{figure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/Multiple/div_by_summary_and_tol.pdf}
        \caption{KL-divergence to true summary distributions by tolerance, taken over 10 trials of the algorithm}
        \label{fig:DivBySummary}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/Multiple/time_by_summary_and_tol.pdf}
        \caption{KL-divergence to true summary distributions by tolerance, taken over 10 trials of the algorithm}
        \label{fig:TimeBySummary}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/Multiple/efficiency_by_summary_and_tol.pdf}
        \caption{KL-divergence to true summary distributions by tolerance, taken over 10 trials of the algorithm}
        \label{fig:EfficiencyBySummary}
    \end{subfigure}
\end{figure}
We see that the KL divergence, runtime, and efficiency of the approximation routine depends on the summary in question.
In particular, the approximation routine for hotspot and coldspot count distributions does not yield as high of an efficiency for moderately low tolerance, and struggles to minimize the KL-divergence to the true distribution for higher tolerances.
This is likely due to the fact that the full hotspot and coldspot count distributions is extremely fast to compute even for large datasets.

These results suggest that convergence and efficiency will vary by summary, and the user should be aware of this fact when choosing whether to run the approximation routine as well as an appropriate tolerance.

\subsection*{Appendix B: Performance of nearest neighbor distribution subsampling algorithm}
This section assesses the performance of the approximation routine \ref{NNDistributionAveraging} modified for the nearest neighbor distribution
We run Algorithm \ref{NNDistributionAveraging} on \texttt{p\_f1} subsampled without replacement to 10,000 sequences.
We compute the nearest neighbor distribution of CDR3 sequences (specified via the \texttt{junction} column) for the full subsampled dataset, as well as using Algorithm \ref{NNDistributionAveraging} with tolerances $\varepsilon \in \left\{0.1, 0.001, \dotsc, 10^{-7} \right\}$.
We replicate this experiment for 10 trials in the same manner as detailed in Appendix A.
Figure~\ref{fig:NNFreqPoly} shows a frequency polygon of the same distributions.
Figure~\ref{fig:NNECDF} shows their empirical cumulative distribution functions, which reveals bias in the tail of the approximation distributions.

\begin{figure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=\linewidth]{Figures/NearestNeighbor/CDR3/freqpoly_by_tol.pdf}
   		\caption{Frequency polygons of true and subsampled nearest neighbor distance distributions by tolerance.}
    	\label{fig:NNFreqPoly}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=\linewidth]{Figures/NearestNeighbor/CDR3/ecdf_by_tol.pdf}
    	\caption{Empirical c.d.f. of true and subsampled nearest neighbor distance distributions by tolerance.}
    	\label{fig:NNECDF}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=\linewidth]{Figures/NearestNeighbor/CDR3/div_by_tol.pdf}
    	\caption{KL-divergence to true nearest neighbor distance distribution by tolerance, taken over 10 trials of the algorithm.}
    	\label{fig:NNDivergences}
	\end{subfigure}
    \begin{subfigure}{.49\textwidth}
    	\includegraphics[width=0.9\linewidth]{Figures/NearestNeighbor/CDR3/log_time_by_tol.pdf}
    	\caption{Runtime (in seconds) and log-runtime (in log-seconds) for Algorithm \ref{DistributionAveraging} by tolerance, taken over 10 trials.}
    	\label{fig:NNTimes}
    \end{subfigure}
\end{figure}

To assess the effect of sequence lengths on Algorithm \ref{NNDistributionAveraging}, we perform the same experiment as above on pairwise aligned query sequences rather than inferred CDR3 sequences.
These length distributions are different by about an order of magnitude.
We note that the \texttt{sequence\_alignment} column is the default for Algorithm \ref{NNDistributionAveraging} within \texttt{sumrep}, although we anticipate users to examine this distribution for CDR3s as well as full V(D)J sequences.
We run Algorithm \ref{NNDistributionAveraging} on the same subsampled 10,000 sequences of \texttt{p\_f1}.
We compute the nearest neighbor distribution of query sequences for the full subsampled dataset, as well as using Algorithm \ref{NNDistributionAveraging} with tolerances $\varepsilon \in \left\{0.1, 0.001, \dotsc, 10^{-5} \right\}$.
We replicate this experiment for 10 trials in the same manner as detailed in Appendix A.
Figure~\ref{fig:NNFreqPolySequence} shows a frequency polygon of the same distributions, and figure \ref{fig:NNECDFSequence} shows their empirical cumulative distribution functions.
Moreover, figures \ref{fig:NNDivergencesSequence}
 and \ref{fig:NNTimesSequence} show the KL-divergences to truth and runtimes, respectively.
\begin{figure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=\linewidth]{Figures/NearestNeighbor/Sequence/freqpoly_by_tol.pdf}
   		\caption{Frequency polygons of true and subsampled nearest neighbor distance distributions by tolerance.}
    	\label{fig:NNFreqPolySequence}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=\linewidth]{Figures/NearestNeighbor/Sequence/ecdf_by_tol.pdf}
    	\caption{Empirical c.d.f. of true and subsampled nearest neighbor distance distributions by tolerance.}
    	\label{fig:NNECDFSequence}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \includegraphics[width=\linewidth]{Figures/NearestNeighbor/Sequence/div_by_tol.pdf}
    	\caption{KL-divergence to true nearest neighbor distance distribution by tolerance, taken over 10 trials of the algorithm.}
    	\label{fig:NNDivergencesSequence}
	\end{subfigure}
    \begin{subfigure}{.49\textwidth}
    	\includegraphics[width=0.9\linewidth]{Figures/NearestNeighbor/Sequence/log_time_by_tol.pdf}
    	\caption{Runtime (in seconds) and log-runtime (in log-seconds) for Algorithm \ref{DistributionAveraging} by tolerance, taken over 10 trials.}
    	\label{fig:NNTimesSequence}
    \end{subfigure}
\end{figure}

Next we investigate the effect of dataset size on the performance of Algorithm \ref{NNDistributionAveraging}.
For sample sizes $s \in \{\exp(6), \dots, \exp(10)\}$, we subsample \texttt{p\_f1} without replacement to $s$ sequences and compute the pairwise distance distribution of \texttt{junction} sequences for the full subsampled dataset as well as those given by tolerances $\varepsilon \in \{0.1, ..., 10^{-5}\}$.
We perform this experiment 5 times for each $s$.

Figures \ref{fig:NNDivBySizeCDR3}, \ref{fig:NNTimeBySizeCDR3}, and \ref{fig:NNEfficiencyBySizeFull} display boxplots of the KL-divergence to truth, runtime, and time efficiency, respectively.
\begin{figure}
	\begin{subfigure}{0.5\textwidth}
    	\includegraphics[width=\linewidth]{Figures/NearestNeighbor/CDR3/div_by_size_and_tol.pdf}
    	\caption{KL-divergence to true nearest neighbor distribution by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
    	\label{fig:NNDivBySizeCDR3}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
    	\includegraphics[width=\linewidth]{Figures/NearestNeighbor/CDR3/time_by_size_and_tol.pdf}
    	\caption{Time complexity of the approximate nearest neighbor distribution by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
    	\label{fig:NNTimeBySizeCDR3}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/NearestNeighbor/CDR3/efficiency_by_size_and_tol.pdf}
    	\caption{KL-divergence to true nearest neighbor distribution by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
    	\label{fig:NNEfficiencyBySizeCDR3}
    \end{subfigure}
\end{figure}
There is not an obvious trend in KL divergence to truth for a given tolerance as sample size increases, although the variability is higher for high tolerances.
As expected, runtime increases as tolerance decreases, and also increases with the size of the dataset.
This is reasonable since each batch iteration of \ref{NNDistributionAveraging} must compute the nearest neighbor distance from each sequence in batch $B$ to the full repertoire $R$, which is $\mathcal O(n^2 m)$, where $n = \text{card}(R)$ is the number of sequences in the full repertoire, and $m = \max_{s \in B} \text{length}(s)$ is the largest sequence length in the batch sample.

Next we look at the efficiency relative to computing the full distribution.
Note the log-log scale, so that the line $y=0$ corresponds to instances when the true and approximate routines have identical runtimes.
Thus, the region $y > 0$ corresponds to instances when Algorithm \ref{NNDistributionAveraging} outperforms the computation of the full nearest neighbor distribution.
We see that efficiency increases with dataset size as well as tolerance.
The former verifies that the complexity of the approximation routine is lower than that of computing the full distirbution.
Examining the boxplots near $y = 0$ by $\log(\text{size})$, we see that for a dataset of size $\exp(k)$, we would need a tolerance of at least $\frac{1}{10^{k - 4}}$.
For example, for $\log(\text{size}) = 6$, we see that tolerances higher than $0.01 = \frac{1}{100} = \frac{1}{10^{6 - 4}}$ would on average yield an efficiency greater than one.
This suggests that, for a dataset with $n$ CDR3 sequences, a sensible rule of thumb would be to choose $\varepsilon > \frac{1}{10^{k - 4}} = \frac{1}{10^{\log(n) - 4}}$.
This will of course be more or less appropriate for a given dataset depending on the nature of the repertoire from which it was sampled.

Finally, we perfrom the same experiment but using \texttt{sequence\_alignment} sequences for the nearest neighbor distance distribution.
Figures \ref{fig:NNDivBySizeFull}, \ref{fig:NNTimeBySizeFull}, and \ref{fig:NNEfficiencyBySizeFull} display boxplots of the KL-divergence to truth, runtime, and time efficiency, respectively.
\begin{figure}
	\begin{subfigure}{0.5\textwidth}
    	\includegraphics[width=\linewidth]{Figures/NearestNeighbor/div_by_size_and_tol.pdf}
    	\caption{KL-divergence to true nearest neighbor distribution by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
    	\label{fig:NNDivBySizeFull}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
    	\includegraphics[width=\linewidth]{Figures/NearestNeighbor/time_by_size_and_tol.pdf}
    	\caption{Time complexity of the approximate nearest neighbor distribution by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
    	\label{fig:NNTimeBySizeFull}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \includegraphics[width=\linewidth]{Figures/NearestNeighbor/efficiency_by_size_and_tol.pdf}
    	\caption{KL-divergence to true nearest neighbor distribution by tolerance and log(size) of dataset, taken over 10 trials of the algorithm.}
    	\label{fig:NNEfficiencyBySizeFull}
    \end{subfigure}
\end{figure}
There is evidence of a positive trend of the KL-divergence as sample size increases for $\varepsilon = 0.1$, although seems to diminish for each other tolerance.
Runtimes increase with given sample size and tolerance, and are generally higher than they are for \texttt{junction} sequences as expected.
It turns out that the efficiencies follow the same rule of thumb we derived for the \texttt{junction} sequence situation.
In particular, choosing $\varepsilon > \frac{1}{10^{k - 4}} = \frac{1}{10^{\log(n) - 4}}$ will on average lead to an increase in efficiency with respect to the full distribution for \texttt{sequence\_alignment} sequences as well as \texttt{junction} sequences.
While this may depend on the dataset in question, we recommend this as a good point of reference for general use.

The user should use these results as well as problem-specific considerations when deciding whether or not to use Algorithm \ref{NNDistributionAveraging} instead of computing the full distribution, and if so, which tolerance to use.

\subsection*{Appendix C: Model validation analysis workflows}

Figure~\ref{fig:PartisWorkflow} describes the overall workflow of this procedure for a given BCR dataset.

Figure~\ref{fig:IgorWorkflow} describes the workflow of this procedure for a given TCR dataset, which is similar to \ref{fig:PartisWorkflow}; the main difference is that we must employ IgBlast to obtain CDR3 sequences for the observed sequences, which \texttt{igor} only outputs for generated sequences.
%EM Hm, isn't CDR3 pretty obvious? (I'm not sure but that was my impression)
%BJO - That was my thought too, but I'm not 100% sure these two tools would always infer the same CDR3.
% Hence, the resultant analysis can be thought to validate a joint modeling effort between \texttt{igor} and IgBlast to whatever extent IgBlast introduces bias in CDR3 inferences.
\begin{figure}
    \begin{subfigure}{0.5\textwidth}
    \begin{adjustbox}{max totalsize={\textwidth}{.5\textheight},center}
        \begin{tikzpicture}
            \node[block](fasta){Fasta file};
            \node[cloud, below =0.5 cm of fasta, align=center](partis){\texttt{partis}\\ \texttt{partition}};
            \node[block, below left=0.75 cm and 0.1 cm of partis](partisdata){Annotated observations};
            \node[block, below = 0.75 cm of partisdata](clones){Subsample to unique clones};
            \node[block, below right=0.75 cm and 0.1 cm of partis](params){Model parameters};
            \node[cloud, below=0.75cm of params, align=center](simulate){\texttt{partis} \\ \texttt{simulate}};
            \node[block, below = 0.75cm of simulate](sim){Annotated simulations};
            \node[block, below = 0.75cm of sim](simclones){Subsample to unique clones};
            \node[block, below left=0.75 and 0.1cm of simclones](c1){Comparisons};
            \path[line](fasta) -- (partis);
            \path[line] (partis) -- (partisdata);
            \path[line] (partis) -- (params);
            \path[line] (params) -- (simulate);
            \path[line] (simulate) -- (sim);
            \path[line] (partisdata) -- (clones);
            \path[line] (clones) -- (c1);
            \path[line] (sim) -- (simclones);
            \path[line] (simclones) -- (c1);
        \end{tikzpicture}
    \end{adjustbox}
    \caption{Workflow for comparing a given observed repertoire dataset to an example simulated dataset via \texttt{partis}.}
    \label{fig:PartisWorkflow}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
    \begin{adjustbox}{max totalsize={\textwidth}{.5\textheight},center}
        \begin{tikzpicture}
            \node[block](fasta){Fasta file};
            \node[cloud, below =0.5 cm of fasta, align=center](igor){\texttt{igor}\\ \texttt{-infer}};
            \node[block, below left=0.75 cm and 0.2 cm of igor](igordata){Annotated observations};
            \node[cloud, below=0.75cm of igordata, align=center](igblast){\texttt{igblast}\\ (for CDR3s)} ;
            \node[block, below right=0.75 cm and 0.2 cm of igor](params){Model parameters};
            \node[cloud, below=0.75cm of params, align=center](simulate){\texttt{igor} \\ \texttt{-generate}};
            \node[block, below = 0.75cm of simulate](sim){Annotated simulations};
            \node[block, below left=0.75 cm  and 0 cm of sim](c1){Comparisons};
            \path[line](fasta) -- (igor);
            \path[line] (igor) -- (igordata);
            \path[line] (igor) -- (params);
            \path[line] (params) -- (simulate);
            \path[line] (simulate) -- (sim);
            \path[line] (igordata) -- (igblast);
            \path[line] (igblast) -- (c1);
            \path[line] (sim) -- (c1);
        \end{tikzpicture}
    \end{adjustbox}
    \caption{Workflow for comparing a given observed repertoire dataset to an example simulated dataset via \texttt{igor}.}
    \label{fig:IgorWorkflow}
    \end{subfigure}
\end{figure}



\subsection*{Appendix D: Comparison of summary scores using IgBlast annotations}

Change-O was used to parse the \texttt{igblast} output \cite{Gupta2015-iu}.
Our strategy is analogous to the one above but with the inclusion of a separate path for \texttt{igblast} annotations.
The workflow for this procedure is displayed in figure \ref{IgblastWorkflow}.
\begin{figure}
    \begin{adjustbox}{max totalsize={\textwidth}{.8\textheight},center}
        \begin{tikzpicture}
        \node[block](fasta){Fasta file};
        \node[cloud, below left=0.5 cm and 1.2 cm of fasta, align=center](partis){\texttt{partis}\\ \texttt{partition}};
        \node[block, below left=0.75 cm and 0.1 cm of partis](partisdata){Annotated observations};
        \node[block, below = 0.75 cm of partisdata](clones){Subsample to unique clones};
        \node[block, below right=0.75 cm and 0.1 cm of partis](params){Model parameters};
        \node[cloud, below=0.75cm of params, align=center](simulate){\texttt{partis} \\ \texttt{simulate}};
        \node[block, below = 0.75cm of simulate](sim){Annotated simulations};
        \node[block, below = 0.75cm of sim](simclones){Subsample to unique clones};
        \node[block, below left=0.75 and 0.1cm of simclones](c1){Comparisons};
        \path[line](fasta) -- (partis);
        \path[line] (partis) -- (partisdata);
        \path[line] (partis) -- (params);
        \path[line] (params) -- (simulate);
        \path[line] (simulate) -- (sim);
        \path[line] (partisdata) -- (clones);
        \path[line] (clones) -- (c1);
        \path[line] (sim) -- (simclones);
        \path[line] (simclones) -- (c1);
            \node[cloud, below right=0.5 cm and 1.2 cm of fasta, align=center](igblast){\texttt{igblast} + \\ \texttt{Change-O}};
            \node[block, below=0.6 cm of igblast](igblastdata){Annotated observations};
            \node[block, below = 0.75 cm of igblastdata](igblastclones){Subsample to unique clones};
            \node[block, below right=0.75 and 0.1cm of simclones](c2){Comparisons};
            \path[line](fasta) -- (igblast);
            \path[line] (igblast) -- (igblastdata);
            \path[line] (igblastdata) -- (igblastclones);
            \path[line] (igblastclones) -- (c2);
            \path[line] (simclones) -- (c2);
        \end{tikzpicture}
    \end{adjustbox}
    \caption{Workflow for comparing \texttt{partis} and \texttt{igblast} annotations to partis simulations}
    \label{IgblastWorkflow}
\end{figure}
The results are displayed in figure \ref{fig:IgBlastScores}.
\begin{figure}
	\begin{subfigure}{\textwidth}
   		\includegraphics[width=\linewidth]{Figures/PartisScores/obs_score_plot_igb.pdf}
    	\caption{Comparing divergences for \texttt{igblast} annotations and \texttt{partis} simulations based on the same individual observed repertoires.
    	    We use the default germline databases in \texttt{igblast} for consistency.
    	}
    	\label{fig:IgBlastScores}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
   		\includegraphics[width=\linewidth]{Figures/PartisScores/score_diff.pdf}
    	\caption{Difference in score$_\text{obs}$ values when using \texttt{igblast} versus \texttt{partis} for annotations.
    	}
    	\label{fig:ScoreDiffs}
	\end{subfigure}
\end{figure}
The plots show a high level of agreement for most summaries, with all but six of them differing by less than one units, and a strong majority of them close to zero.
Where differences arise, this is likely the result of differences in how partis and IgBlast perform annotations.
For example, we see that the insertion length distributions highly disagree in scores.
This is at least partially attributable to the star-tree assumption on which partis operates, which is prone to overestimate insertion lengths in an effort to better estimate the ultimate naive sequence.
Indeed, examining the VD insertion length distribution shows that IgBlast tends to assign a similar distribution to each dataset, whereas partis leads to more variable distributions with right skew due to the star-tree assumption.
Moreover, if IgBlast tends to assign a similar insertion length distribution to every dataset, then this will make it difficult for a simulator designed to match particular insertion lengths distributions to behave more like the IgBlast distributions.
Thus, inherent differences in annotation tools will certainly lead to differences in summary scores, regardless of how accurate either tool is.
Hence, it is important to understand that a given annotations-based summary should be considered in the context of the tool which provided annotations, and not as a ground-truth summary of the actual gene usage/indel statistics.



\begin{figure}
    \includegraphics[width=\linewidth]{Figures/PartisScores/pi_freqpoly.pdf}
    \caption{Summary distribution frequency polygons of three datasets based on \texttt{partis} versus \texttt{igblast} annotations.}
\end{figure}

\begin{figure}
    \includegraphics[width=\linewidth]{Figures/PartisScores/pi_ecdf.pdf}
    \caption{Summary distribution empirical cdfs of three datasets based on \texttt{partis} versus \texttt{igblast} annotations.}
\end{figure}

\section*{Appendix E: Metadata for partis analysis}

Table \ref{tab:Datasets} gives metadata for each dataset used in the following analyses.
In short, the notation is as follows.
A prefix of \texttt{p} corresponds to \texttt{partis} annotations, a prefix of \texttt{i} corresponds to \texttt{igblast} annotations, and a prefix of \texttt{pi} corresponds to \texttt{partis} annotations using the default \texttt{igblast} germline databases;
this is followed by a letter representing the individual succeeded by a number corresponding to the time point;
finally, we append the \texttt{sim} suffix if the dataset was simulated from an individual's inferred model parameters.
\begin{table}
\makebox[\textwidth][c]{
\begin{tabular}{c|c|c|c|c|c}
	Dataset label & Dataset type & Annotation tool & Germline set & Individual & Time point \\
	\hline
    \texttt{p\_f1} & Observation &     \texttt{partis} & \texttt{partis} & F & 1 hour \\
    \texttt{p\_f2} & Observation &     \texttt{partis} & \texttt{partis} & F & 8 days \\
    \texttt{p\_g1} & Observation &     \texttt{partis} & \texttt{partis} & G & 1 hour \\
    \texttt{p\_g2} & Observation &     \texttt{partis} & \texttt{partis} & G & 8 days \\
    \texttt{p\_i1} & Observation &     \texttt{partis} & \texttt{partis} & I & 1 hour \\
    \texttt{p\_i2} & Observation &     \texttt{partis} & \texttt{partis} & I & 8 days \\
    \texttt{i\_f1} & Observation &     \texttt{igblast} & IMGT & F & 1 hour \\
    \texttt{i\_f2} & Observation &     \texttt{igblast} & IMGT & F & 8 days \\
    \texttt{i\_g1} & Observation &     \texttt{igblast} & IMGT & G & 1 hour\\
    \texttt{i\_g2} & Observation &     \texttt{igblast} & IMGT & G & 8 days\\
    \texttt{i\_i1} & Observation &     \texttt{igblast} & IMGT & I & 1 hour\\
    \texttt{i\_i2} & Observation &     \texttt{igblast} & IMGT & I & 8 days\\
    \texttt{pi\_f1} & Observation &    \texttt{partis} & IMGT & F & 1 hour\\
    \texttt{pi\_f2} & Observation &    \texttt{partis} & IMGT & F & 8 days\\
    \texttt{pi\_g1} & Observation &    \texttt{partis} & IMGT & G & 1 hour\\
    \texttt{pi\_g2} & Observation &    \texttt{partis} & IMGT & G & 8 days\\
    \texttt{pi\_i1} & Observation &    \texttt{partis} & IMGT & I & 1 hour\\
    \texttt{pi\_i2} & Observation &    \texttt{partis} & IMGT & I & 8 days\\
    \texttt{p\_f1\_sim} & Simulation & \texttt{partis} & \texttt{partis} & F & 1 hour\\
    \texttt{p\_f2\_sim} & Simulation & \texttt{partis} & \texttt{partis} & F & 8 days\\
    \texttt{p\_g1\_sim} & Simulation & \texttt{partis} & \texttt{partis} & G & 1 hour\\
    \texttt{p\_g2\_sim} & Simulation & \texttt{partis} & \texttt{partis} & G & 8 days\\
    \texttt{p\_i1\_sim} & Simulation & \texttt{partis} & \texttt{partis} & I & 1 hour\\
    \texttt{p\_i2\_sim} & Simulation & \texttt{partis} & \texttt{partis} & I & 8 days\\
    \texttt{pi\_f1\_sim} & Simulation& \texttt{partis} & IMGT & F & 1 hour\\
    \texttt{pi\_f2\_sim} & Simulation& \texttt{partis} & IMGT & F & 8 days\\
    \texttt{pi\_g1\_sim} & Simulation& \texttt{partis} & IMGT & G & 1 hour\\
    \texttt{pi\_g2\_sim} & Simulation& \texttt{partis} & IMGT & G & 8 days\\
    \texttt{pi\_i1\_sim} & Simulation& \texttt{partis} & IMGT & I & 1 hour\\
    \texttt{pi\_i2\_sim} & Simulation& \texttt{partis} & IMGT & I & 8 days\\
\end{tabular}
}
\caption{Metadata for each dataset used in the analyses.}
\label{tab:Datasets}
\end{table}


\end{document}
